import streamlit as st
import math
import pandas as pd
import altair as alt
from PIL import Image
import os

logo_path = os.path.join("assets", "weaviate-logo.png")
logo_image = Image.open(logo_path)

st.set_page_config(
	page_title="Weaviate Storage Calculator",
	layout="wide",
	initial_sidebar_state="expanded",
	page_icon=logo_image
)

# Apply custom styling
st.markdown("""
            <style>
                .main .block-container {
                    padding-top: 2rem;
                }
                .stTabs [data-baseweb="tab-list"] {
                    gap: 2px;
                }
                .stTabs [data-baseweb="tab"] {
                    height: 50px;
                    white-space: pre-wrap;
                    background-color: #f0f2f6;
                    border-radius: 4px 4px 0 0;
                    gap: 1px;
                    padding-top: 10px;
                    padding-bottom: 10px;
                }
                .stTabs [aria-selected="true"] {
                    background-color: #4e8be6;
                    color: white;
                }
            </style>
            """, unsafe_allow_html=True)

def format_bytes(bytes_value):
	"""Format bytes to human-readable format."""
	if bytes_value < 1024:
		return f"{bytes_value:.2f} B"
	elif bytes_value < 1024 * 1024:
		return f"{bytes_value / 1024:.2f} KB"
	elif bytes_value < 1024 * 1024 * 1024:
		return f"{bytes_value / (1024 * 1024):.2f} MB"
	else:
		return f"{bytes_value / (1024 * 1024 * 1024):.2f} GB"

def calculate_weaviate_storage(
	num_objects,
	vector_dimensions,
	avg_object_size_bytes,
	quantization="none",
):
	"""Calculate Weaviate disk storage based on OFFICIAL documentation formulas only."""

	# Vector storage calculation (OFFICIAL: dimensions Ã— 4 bytes for float32)
	bytes_per_dimension = 4 # 32-bit float
	uncompressed_vector_bytes = vector_dimensions * bytes_per_dimension

	# Apply quantization if specified (based on OFFICIAL examples)
	if quantization == 'sq': # Scalar Quantization: 32-bit to 8-bit
		compressed_vector_bytes = vector_dimensions * 1 # 8-bit = 1 byte
		compression_ratio = 4
		vector_bytes_per_object = compressed_vector_bytes
	elif quantization == 'pq': # Product Quantization
		# OFFICIAL example: 768 dims â†’ 128 segments Ã— 1 byte = 128 bytes
		# This gives us a pattern but no universal formula is documented
		if vector_dimensions == 768:
			segments = 128
		elif vector_dimensions == 1536:
			segments = 128 # Common practice but not officially documented
		else:
			# No official formula provided, using reasonable estimate
			segments = max(1, vector_dimensions // 6)

		compressed_vector_bytes = segments * 1 # 1 byte per segment
		compression_ratio = uncompressed_vector_bytes / compressed_vector_bytes
		vector_bytes_per_object = compressed_vector_bytes
	elif quantization == 'bq': # Binary Quantization: 1 bit per dimension
		compressed_vector_bytes = math.ceil(vector_dimensions / 8) # 1 bit per dimension
		compression_ratio = 32
		vector_bytes_per_object = compressed_vector_bytes
	else:
		vector_bytes_per_object = uncompressed_vector_bytes
		compression_ratio = 1
		compressed_vector_bytes = 0

	total_vector_storage = num_objects * vector_bytes_per_object

	# Object storage (straightforward calculation)
	total_object_storage = num_objects * avg_object_size_bytes

	# Total disk storage
	total_storage_bytes = total_object_storage + total_vector_storage

	return {
		"object_storage_bytes": total_object_storage,
		"vector_storage_bytes": total_vector_storage,
		"total_storage_bytes": total_storage_bytes,
		"compression_ratio": compression_ratio,
		"uncompressed_vector_bytes": uncompressed_vector_bytes,
		"compressed_vector_bytes": compressed_vector_bytes if quantization != "none" else 0,
		"vector_bytes_per_object": vector_bytes_per_object
	}

def extrapolate_from_sample(sample_objects, sample_storage_bytes, target_objects):
	"""Extrapolate storage requirements from a sample dataset."""
	# Simple linear extrapolation
	storage_per_object = sample_storage_bytes / sample_objects
	linear_storage = storage_per_object * target_objects

	# Conservative extrapolation with small overhead for growth
	if target_objects > sample_objects:
		scale_factor = target_objects / sample_objects
		# Add modest overhead for growth
		overhead_factor = 1 + (math.log(scale_factor) / math.log(10)) * 0.05 # 5% per order of magnitude
		conservative_storage = linear_storage * overhead_factor
	else:
		conservative_storage = linear_storage

	return {
		"sample_info": {
			"objects": sample_objects,
			"storage_bytes": sample_storage_bytes,
			"storage_per_object": storage_per_object
		},
		"target_objects": target_objects,
		"linear_extrapolation": {
			"storage_bytes": linear_storage,
			"storage_gb": linear_storage / (1024 * 1024 * 1024)
		},
		"conservative_extrapolation": {
			"storage_bytes": conservative_storage,
			"storage_gb": conservative_storage / (1024 * 1024 * 1024)
		}
	}

# App Header
st.title("Weaviate Storage Calculator")
st.markdown("This calculator uses **official Weaviate documentation formulas** for storage estimats.")

# Create tabs
tab1, tab2 = st.tabs(["ðŸ“Š Parameter-Based", "ðŸ“ˆ Extrapolate from current Dataset"])

with tab1:
	st.header("Storage Calculation")

	col1, col2 = st.columns(2)

	with col1:
		st.subheader("Object Parameters")

		num_objects = st.number_input(
			"Number of Objects", 
			min_value=1,
			value=1000000,
			step=10000,
			help="Total number of objects to be stored in Weaviate"
		)

		object_size_unit = st.selectbox(
			"Object Size Unit",
			options=["Bytes", "Kilobytes (KB)", "Megabytes (MB)"],
			index=1,
			help="Unit for specifying object size"
		)

		avg_object_size = st.number_input(
			f"Average Object Size ({object_size_unit.split()[0]})",
			min_value=0.001,
			value=5.0,
			step=1.0,
			help="Average size of each object (without vectors)"
		)

		# Convert to bytes based on selected unit
		if object_size_unit == "Kilobytes (KB)":
			avg_object_size_bytes = avg_object_size * 1024
		elif object_size_unit == "Megabytes (MB)":
			avg_object_size_bytes = avg_object_size * 1024 * 1024
		else:
			avg_object_size_bytes = avg_object_size

	with col2:
		st.subheader("Vector Parameters")

		vector_dimensions = st.number_input(
			"Vector Dimensions", 
			min_value=1, 
			value=1536,
			step=128,
			help="Dimensionality of vector embeddings (e.g., 768 for BERT, 1536 for OpenAI)"
		)

		quantization = st.selectbox(
			"Vector Quantization Method",
			options=[
				"None (32-bit float)",
				"Scalar Quantization (SQ)",
				"Product Quantization (PQ)",
				"Binary Quantization (BQ)"
			],
			index=0,
			help="Vector compression method"
		)

		quant_map = {
			"None (32-bit float)": "none",
			"Scalar Quantization (SQ)": "sq",
			"Product Quantization (PQ)": "pq", 
			"Binary Quantization (BQ)": "bq"
		}
		quantization_code = quant_map[quantization]

		st.markdown("---")

		# Display vector size information
		uncompressed_size = vector_dimensions * 4
		st.metric(
			"Uncompressed Vector Size", 
			f"{format_bytes(uncompressed_size)}",
			help="Official: dimensions Ã— 4 bytes (32-bit float)"
		)

		# Show compressed size if quantization is selected
		if quantization_code != "none":
			if quantization_code == 'sq':
				compressed_size = vector_dimensions * 1
			elif quantization_code == 'pq':
				if vector_dimensions == 768:
					segments = 128
				elif vector_dimensions == 1536:
					segments = 128
				else:
					segments = max(1, vector_dimensions // 6)
				compressed_size = segments * 1
			elif quantization_code == 'bq':
				compressed_size = math.ceil(vector_dimensions / 8)

			st.metric(
				"Compressed Vector Size",
				f"{format_bytes(compressed_size)}",
				help="Size after quantization"
			)

	# Calculate storage
	storage_results = calculate_weaviate_storage(
		num_objects=num_objects,
		vector_dimensions=vector_dimensions,
		avg_object_size_bytes=avg_object_size_bytes,
		quantization=quantization_code
	)

	st.markdown("---")

	# Display storage breakdown
	st.subheader("Storage Components")

	col1, col2, col3 = st.columns(3)

	with col1:
		st.metric(
			"Object Storage",
			f"{format_bytes(storage_results['object_storage_bytes'])}",
			help="Raw object data storage"
		)

	with col2:
		st.metric(
			"Vector Storage",
			f"{format_bytes(storage_results['vector_storage_bytes'])}",
			help="Vector embeddings (compressed if quantization enabled)"
		)

	with col3:
		st.metric(
			"Total Storage",
			f"{format_bytes(storage_results['total_storage_bytes'])}",
			f"{storage_results['total_storage_bytes'] / (1024**3):.2f} GB",
			help="Sum of officially documented storage components"
		)

	# Compression details
	if quantization_code != "none":
		st.subheader("Compression Details")

		col1, col2, col3 = st.columns(3)

		with col1:
			st.metric(
				"Compression Ratio",
				f"{storage_results['compression_ratio']:.1f}:1",
				help="Storage reduction ratio"
			)

		with col2:
			savings_percent = (1 - 1/storage_results['compression_ratio']) * 100
			st.metric(
				"Storage Savings",
				f"{savings_percent:.1f}%",
				help="Percentage reduction in vector storage"
			)

		with col3:
			st.metric(
				"Bytes per Vector",
				f"{storage_results['vector_bytes_per_object']} bytes",
				help="Storage per vector after compression"
			)

	# Storage Components Visualization
	storage_data = pd.DataFrame({
		'Component': ['Objects', 'Vectors'],
		'Size (GB)': [
			storage_results['object_storage_bytes'] / (1024**3),
			storage_results['vector_storage_bytes'] / (1024**3)
		]
	})

	chart = alt.Chart(storage_data).mark_bar().encode(
		x=alt.X('Component:N', title=None),
		y=alt.Y('Size (GB):Q', title='Storage (GB)'),
		color=alt.Color('Component:N', scale=alt.Scale(scheme='blues'), legend=None),
		tooltip=['Component', alt.Tooltip('Size (GB):Q', format='.2f')]
	).properties(
		title='Disk Storage Components (Official Formulas Only)',
		width=600,
		height=300
	)

	st.altair_chart(chart, use_container_width=True)

	# Quantization Comparison
	st.subheader("Quantization Methods Comparison")
	
	# Calculate storage for all quantization methods
	quantization_methods = ["none", "sq", "pq", "bq"]
	method_names = ["None (32-bit float)", "Scalar Quantization (SQ)", "Product Quantization (PQ)", "Binary Quantization (BQ)"]
	
	comparison_data = []
	for method in quantization_methods:
		results = calculate_weaviate_storage(
			num_objects=num_objects,
			vector_dimensions=vector_dimensions, 
			avg_object_size_bytes=avg_object_size_bytes,
			quantization=method
		)
		comparison_data.append({
			'Method': method_names[quantization_methods.index(method)],
			'Vector Storage (GB)': results['vector_storage_bytes'] / (1024**3),
			'Total Storage (GB)': results['total_storage_bytes'] / (1024**3),
			'Compression Ratio': f"{results['compression_ratio']:.1f}:1"
		})
	
	comparison_df = pd.DataFrame(comparison_data)
	
	# Display as table
	st.dataframe(comparison_df, use_container_width=True, hide_index=True)
	
	# Visualization
	chart_comparison = alt.Chart(comparison_df).mark_bar().encode(
		x=alt.X('Method:N', title='Quantization Method', axis=alt.Axis(labelAngle=-45)),
		y=alt.Y('Total Storage (GB):Q', title='Total Storage (GB)'),
		color=alt.Color('Method:N', scale=alt.Scale(scheme='category10'), legend=None),
		tooltip=['Method', alt.Tooltip('Total Storage (GB):Q', format='.2f'), 'Compression Ratio']
	).properties(
		title='Storage Comparison Across Quantization Methods',
		width=600,
		height=300
	)

	st.altair_chart(chart_comparison, use_container_width=True)

	with st.expander("Documentation Notes"):
		st.markdown("""
		            ### Weaviate Documentation
		            
		            **Formulas** :
		            - **Vector Storage**: `dimensions Ã— 4 bytes` for 32-bit floats
		            - **Object Storage**: Raw object data storage
		            
		            **Compression Examples from Documentation**:
		            - **PQ**: 768-dim vector: 3,072 bytes â†’ 128 bytes (24:1 ratio)
		            - **SQ**: 32-bit â†’ 8-bit reduction (4:1 ratio)
		            - **BQ**: 32-bit â†’ 1-bit reduction (32:1 ratio)
		            
		            This calculator uses only disk storage formulas documented by Weaviate.
		            """)

with tab2:
	st.header("Extrapolate from your current Dataset")
	st.markdown("Use actual measurements from your current Weaviate dataset to predict storage growth.")

	col1, col2 = st.columns(2)

	with col1:
		st.subheader("Sample Dataset")

		sample_objects = st.number_input(
			"Objects count in Dataset", 
			min_value=1,
			value=10000,
			step=1000,
			help="Number of objects in your dataset"
		)

		sample_size_unit = st.selectbox(
			"Storage Unit",
			options=["Bytes", "Kilobytes (KB)", "Megabytes (MB)", "Gigabytes (GB)"],
			index=2,
			help="Unit for storage size"
		)

		sample_storage = st.number_input(
			f"Current Storage Size ({sample_size_unit.split()[0]})",
			min_value=0.001,
			value=500.0,
			step=10.0,
			help="Total storage used by your current dataset"
		)

		# Convert to bytes
		if sample_size_unit == "Bytes":
			sample_storage_bytes = sample_storage
		elif sample_size_unit == "Kilobytes (KB)":
			sample_storage_bytes = sample_storage * 1024
		elif sample_size_unit == "Megabytes (MB)":
			sample_storage_bytes = sample_storage * 1024 * 1024
		else: # GB
			sample_storage_bytes = sample_storage * 1024 * 1024 * 1024

	with col2:
		st.subheader("Target Growth")

		target_objects = st.number_input(
			"Expected Number of Objects", 
			min_value=1,
			value=1000000,
			step=100000,
			help="Number of objects to grow to in your dataset"
		)

		storage_per_object = sample_storage_bytes / sample_objects
		st.metric(
			"Storage size per Object",
			f"{format_bytes(storage_per_object)}",
			help="Average storage size per object in current dataset"
		)

		scale_factor = target_objects / sample_objects
		st.metric(
			"Scale Factor",
			f"{scale_factor:.2f}x",
			help="Size increase from current dataset to target growth"
		)

	# Calculate extrapolation
	extrapolation_results = extrapolate_from_sample(
		sample_objects=sample_objects,
		sample_storage_bytes=sample_storage_bytes,
		target_objects=target_objects
	)

	st.markdown("---")

	# Display results
	st.subheader("Extrapolation Results")

	col1, col2 = st.columns(2)

	with col1:
		st.metric(
			"Linear Extrapolation",
			f"{extrapolation_results['linear_extrapolation']['storage_gb']:.2f} GB",
			help="Direct proportional scaling"
		)

		st.markdown("Simple linear scaling assumes storage grows proportionally with object count.")

	with col2:
		st.metric(
			"Conservative Extrapolation", 
			f"{extrapolation_results['conservative_extrapolation']['storage_gb']:.2f} GB",
			help="Linear + small overhead for larger datasets"
		)

		st.markdown("Adds modest overhead for larger dataset to account for indexing inefficiencies.")

st.markdown("---")
st.caption("**Based on Weaviate Documentation** - This calculator uses formulas documented at weaviate.io.")
